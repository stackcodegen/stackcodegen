question_id,question,answer
20076195,"What is the most efficient way of counting occurrences in pandas?
<p>I have a large (about 12M rows) DataFrame <code>df</code>:</p>
<pre><code>df.columns = ['word','documents','frequency']
</code></pre>
<p>The following ran in a timely fashion:</p>
<pre><code>word_grouping = df[['word','frequency']].groupby('word')
MaxFrequency_perWord = word_grouping[['frequency']].max().reset_index()
MaxFrequency_perWord.columns = ['word','MaxFrequency']
</code></pre>
<p>However, this is taking an unexpectedly long time to run:</p>
<pre><code>Occurrences_of_Words = word_grouping[['word']].count().reset_index()
</code></pre>
<p>What am I doing wrong here?  Is there a better way to count occurrences in a large DataFrame?</p>
<pre><code>df.word.describe()
</code></pre>
<p>ran pretty well, so I really did not expect this <code>Occurrences_of_Words</code> DataFrame to take very long to build.</p>
","<p>I think <code>df['word'].value_counts()</code> should serve. By skipping the groupby machinery, you'll save some time. I'm not sure why <code>count</code> should be much slower than <code>max</code>. Both take some time to avoid missing values. (Compare with <code>size</code>.)</p>

<p>In any case, value_counts has been <a href=""https://github.com/pydata/pandas/commit/d1a1fa27e520c0d196981df58edb1bd5d3cc3161"">specifically optimized</a> to handle object type, like your words, so I doubt you'll do much better than that.</p>
"
